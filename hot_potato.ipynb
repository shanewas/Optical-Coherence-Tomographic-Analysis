{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hot_potato.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"2xEhdIE08WOB","colab_type":"code","outputId":"679d3438-7fd7-4c3d-fc63-00d56b7240f0","executionInfo":{"status":"ok","timestamp":1564685614426,"user_tz":-360,"elapsed":1202683,"user":{"displayName":"SHANEWAS AHMED NABIL","photoUrl":"","userId":"17864818584441031388"}},"colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["import matplotlib.pyplot as plt\n","import PIL\n","import tensorflow as tf\n","import numpy as np\n","import os\n","\n","from tensorflow.python.keras.models import Model, Sequential\n","from tensorflow.python.keras.layers import Dense, Flatten, Dropout\n","from tensorflow.python.keras.applications.vgg19 import VGG19\n","from tensorflow.python.keras.applications.inception_v3 import preprocess_input, decode_predictions\n","from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.python.keras.optimizers import Adam, RMSprop\n","\n","train_dir = \"/content/drive/My Drive/ADNI_C/test/des/ADtrain\"\n","test_dir = \"/content/drive/My Drive/ADNI_C/test/des/ADtest\"\n","\n","model = VGG19(include_top=False,input_shape=(224, 224, 3), weights='imagenet')\n","input_shape = (224,224)\n","\n","datagen_train = ImageDataGenerator(rescale=1./255)\n","datagen_test = ImageDataGenerator(rescale=1./255)\n","\n","\n","batch_size = 20\n","\n","generator_train = datagen_train.flow_from_directory(directory=train_dir,\n","                                                    target_size=input_shape,\n","                                                    batch_size=batch_size,\n","                                                    shuffle=True)\n","\n","generator_test = datagen_test.flow_from_directory(directory=test_dir,\n","                                                  target_size=input_shape,\n","                                                  batch_size=batch_size,\n","                                                  shuffle=False)\n","\n","steps_test = generator_test.n / batch_size\n","\n","\n","def path_join(dirname, filenames):\n","    return [os.path.join(dirname, filename) for filename in filenames]\n","\n","image_paths_train = path_join(train_dir, generator_train.filenames)\n","image_paths_test = path_join(test_dir, generator_test.filenames)\n","\n","cls_train = generator_train.classes\n","cls_test = generator_test.classes\n","\n","\n","class_names = list(generator_train.class_indices.keys())\n","num_classes = generator_train.num_classes\n","\n","transfer_layer = model.get_layer('block5_pool')\n","conv_model = Model(inputs=model.input, outputs=transfer_layer.output)\n","\n","#for layer in conv_model.layers:\n","#    layer.trainable = False\n","    \n","# Start a new Keras Sequential model.\n","new_model = Sequential()\n","\n","# Add the convolutional part of the VGG16 model from above.\n","new_model.add(conv_model)\n","\n","# Flatten the output of the VGG16 model because it is from a\n","# convolutional layer.\n","new_model.add(Flatten())\n","\n","# Add a dense (aka. fully-connected) layer.\n","# This is for combining features that the VGG16 model has\n","# recognized in the image.\n","new_model.add(Dropout(0.5))\n","\n","new_model.add(Dense(1024, activation='relu'))\n","\n","# Add a dropout-layer which may prevent overfitting and\n","# improve generalization ability to unseen data e.g. the test-set.\n","#new_model.add(Dense(512, activation='relu'))\n","\n","\n","# Add the final layer for the actual classification.\n","new_model.add(Dense(num_classes, activation='softmax'))\n","\n","optimizer = Adam(lr=1e-5)\n","loss = 'categorical_crossentropy'\n","metrics = ['categorical_accuracy']\n","\n","\n","def print_layer_trainable():\n","    for layer in conv_model.layers:\n","        print(\"{0}:\\t{1}\".format(layer.trainable, layer.name))\n","\n","\n","print_layer_trainable()\n","\n","new_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n","\n","epochs = 2\n","steps_per_epoch = 100\n","\n","\n","history = new_model.fit_generator(generator=generator_train,\n","                                  epochs=epochs,\n","                                  steps_per_epoch=steps_per_epoch,\n","                                  validation_data=generator_test,\n","                                  validation_steps=steps_test)\n"," "],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0804 18:47:51.726320 139917667190656 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n"],"name":"stderr"},{"output_type":"stream","text":["Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n","80142336/80134624 [==============================] - 3s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"184X1OLS9a4G","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dlQALPBt1LUL","colab_type":"code","outputId":"bfb6f70f-e1c8-451f-bf55-e8e9a0bf5e9c","executionInfo":{"status":"ok","timestamp":1564763067159,"user_tz":-360,"elapsed":5279683,"user":{"displayName":"SHANEWAS AHMED NABIL","photoUrl":"","userId":"17864818584441031388"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import matplotlib.pyplot as plt\n","import PIL\n","import tensorflow as tf\n","import numpy as np\n","import os\n","\n","from tensorflow.python.keras.models import Model, Sequential\n","from tensorflow.python.keras.layers import Dense, Flatten, Dropout\n","from tensorflow.python.keras.applications.vgg19 import VGG19\n","from tensorflow.python.keras.applications.inception_v3 import preprocess_input, decode_predictions\n","from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.python.keras.optimizers import Adam, RMSprop\n","\n","train_dir = \"/content/drive/My Drive/ADNI_C/test/des/ADtrain\"\n","test_dir = \"/content/drive/My Drive/ADNI_C/test/des/ADtest\"\n","\n","model = VGG19(include_top=False,input_shape=(224, 224, 3), weights='imagenet')\n","input_shape = (224,224)\n","\n","datagen_train = ImageDataGenerator(rescale=1./255)\n","datagen_test = ImageDataGenerator(rescale=1./255)\n","\n","\n","batch_size = 20\n","\n","generator_train = datagen_train.flow_from_directory(directory=train_dir,\n","                                                    target_size=input_shape,\n","                                                    batch_size=batch_size,\n","                                                    shuffle=True)\n","\n","generator_test = datagen_test.flow_from_directory(directory=test_dir,\n","                                                  target_size=input_shape,\n","                                                  batch_size=batch_size,\n","                                                  shuffle=False)\n","\n","steps_test = generator_test.n / batch_size\n","\n","\n","def path_join(dirname, filenames):\n","    return [os.path.join(dirname, filename) for filename in filenames]\n","\n","image_paths_train = path_join(train_dir, generator_train.filenames)\n","image_paths_test = path_join(test_dir, generator_test.filenames)\n","\n","cls_train = generator_train.classes\n","cls_test = generator_test.classes\n","\n","\n","class_names = list(generator_train.class_indices.keys())\n","num_classes = generator_train.num_classes\n","\n","transfer_layer = model.get_layer('block5_pool')\n","conv_model = Model(inputs=model.input, outputs=transfer_layer.output)\n","\n","#for layer in conv_model.layers:\n","#    layer.trainable = False\n","    \n","# Start a new Keras Sequential model.\n","new_model = Sequential()\n","\n","# Add the convolutional part of the VGG16 model from above.\n","new_model.add(conv_model)\n","\n","# Flatten the output of the VGG16 model because it is from a\n","# convolutional layer.\n","new_model.add(Flatten())\n","\n","# Add a dense (aka. fully-connected) layer.\n","# This is for combining features that the VGG16 model has\n","# recognized in the image.\n","new_model.add(Dropout(0.5))\n","\n","new_model.add(Dense(1024, activation='relu'))\n","\n","# Add a dropout-layer which may prevent overfitting and\n","# improve generalization ability to unseen data e.g. the test-set.\n","#new_model.add(Dense(512, activation='relu'))\n","\n","\n","# Add the final layer for the actual classification.\n","new_model.add(Dense(num_classes, activation='softmax'))\n","\n","optimizer = Adam(lr=1e-5)\n","loss = 'categorical_crossentropy'\n","metrics = ['categorical_accuracy']\n","\n","\n","def print_layer_trainable():\n","    for layer in conv_model.layers:\n","        print(\"{0}:\\t{1}\".format(layer.trainable, layer.name))\n","\n","\n","print_layer_trainable()\n","\n","new_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n","\n","epochs = 20\n","steps_per_epoch = 100\n","\n","\n","history = new_model.fit_generator(generator=generator_train,\n","                                  epochs=epochs,\n","                                  steps_per_epoch=steps_per_epoch,\n","                                  validation_data=generator_test,\n","                                  validation_steps=steps_test)\n"," "],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0802 14:56:29.855085 140442378991488 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n"],"name":"stderr"},{"output_type":"stream","text":["Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n","80142336/80134624 [==============================] - 7s 0us/step\n","Found 6474 images belonging to 2 classes.\n","Found 1618 images belonging to 2 classes.\n","True:\tinput_1\n","True:\tblock1_conv1\n","True:\tblock1_conv2\n","True:\tblock1_pool\n","True:\tblock2_conv1\n","True:\tblock2_conv2\n","True:\tblock2_pool\n","True:\tblock3_conv1\n","True:\tblock3_conv2\n","True:\tblock3_conv3\n","True:\tblock3_conv4\n","True:\tblock3_pool\n","True:\tblock4_conv1\n","True:\tblock4_conv2\n","True:\tblock4_conv3\n","True:\tblock4_conv4\n","True:\tblock4_pool\n","True:\tblock5_conv1\n","True:\tblock5_conv2\n","True:\tblock5_conv3\n","True:\tblock5_conv4\n","True:\tblock5_pool\n","Epoch 1/20\n","100/100 [==============================] - 1973s 20s/step - loss: 0.6673 - categorical_accuracy: 0.6325 - val_loss: 0.5124 - val_categorical_accuracy: 0.7695\n","Epoch 2/20\n","100/100 [==============================] - 991s 10s/step - loss: 0.4698 - categorical_accuracy: 0.7660 - val_loss: 0.3107 - val_categorical_accuracy: 0.8677\n","Epoch 3/20\n","100/100 [==============================] - 1107s 11s/step - loss: 0.3437 - categorical_accuracy: 0.8295 - val_loss: 0.2127 - val_categorical_accuracy: 0.9147\n","Epoch 4/20\n","100/100 [==============================] - 299s 3s/step - loss: 0.2298 - categorical_accuracy: 0.8960 - val_loss: 0.1426 - val_categorical_accuracy: 0.9394\n","Epoch 5/20\n","100/100 [==============================] - 53s 530ms/step - loss: 0.1839 - categorical_accuracy: 0.9195 - val_loss: 0.1418 - val_categorical_accuracy: 0.9382\n","Epoch 6/20\n","100/100 [==============================] - 53s 532ms/step - loss: 0.1374 - categorical_accuracy: 0.9368 - val_loss: 0.0993 - val_categorical_accuracy: 0.9604\n","Epoch 7/20\n","100/100 [==============================] - 53s 532ms/step - loss: 0.1425 - categorical_accuracy: 0.9410 - val_loss: 0.0679 - val_categorical_accuracy: 0.9796\n","Epoch 8/20\n","100/100 [==============================] - 53s 533ms/step - loss: 0.0964 - categorical_accuracy: 0.9569 - val_loss: 0.0567 - val_categorical_accuracy: 0.9790\n","Epoch 9/20\n","100/100 [==============================] - 53s 533ms/step - loss: 0.0717 - categorical_accuracy: 0.9705 - val_loss: 0.0655 - val_categorical_accuracy: 0.9740\n","Epoch 10/20\n","100/100 [==============================] - 53s 533ms/step - loss: 0.1033 - categorical_accuracy: 0.9565 - val_loss: 0.0555 - val_categorical_accuracy: 0.9765\n","Epoch 11/20\n","100/100 [==============================] - 53s 531ms/step - loss: 0.0533 - categorical_accuracy: 0.9824 - val_loss: 0.0441 - val_categorical_accuracy: 0.9802\n","Epoch 12/20\n","100/100 [==============================] - 53s 534ms/step - loss: 0.0567 - categorical_accuracy: 0.9770 - val_loss: 0.0554 - val_categorical_accuracy: 0.9790\n","Epoch 13/20\n","100/100 [==============================] - 53s 533ms/step - loss: 0.0485 - categorical_accuracy: 0.9785 - val_loss: 0.0477 - val_categorical_accuracy: 0.9796\n","Epoch 14/20\n","100/100 [==============================] - 53s 533ms/step - loss: 0.0357 - categorical_accuracy: 0.9850 - val_loss: 0.0292 - val_categorical_accuracy: 0.9895\n","Epoch 15/20\n","100/100 [==============================] - 53s 531ms/step - loss: 0.0327 - categorical_accuracy: 0.9880 - val_loss: 0.0326 - val_categorical_accuracy: 0.9858\n","Epoch 16/20\n","100/100 [==============================] - 53s 534ms/step - loss: 0.0540 - categorical_accuracy: 0.9795 - val_loss: 0.0635 - val_categorical_accuracy: 0.9790\n","Epoch 17/20\n","100/100 [==============================] - 53s 533ms/step - loss: 0.0639 - categorical_accuracy: 0.9795 - val_loss: 0.0211 - val_categorical_accuracy: 0.9932\n","Epoch 18/20\n","100/100 [==============================] - 53s 531ms/step - loss: 0.0281 - categorical_accuracy: 0.9895 - val_loss: 0.0251 - val_categorical_accuracy: 0.9895\n","Epoch 19/20\n","100/100 [==============================] - 53s 533ms/step - loss: 0.0173 - categorical_accuracy: 0.9930 - val_loss: 0.0598 - val_categorical_accuracy: 0.9778\n","Epoch 20/20\n","100/100 [==============================] - 53s 533ms/step - loss: 0.0232 - categorical_accuracy: 0.9925 - val_loss: 0.0172 - val_categorical_accuracy: 0.9932\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"La8QyesNSrQx","colab_type":"code","outputId":"f83ff756-920d-4c95-a249-72371c9952bf","executionInfo":{"status":"ok","timestamp":1564819834651,"user_tz":-360,"elapsed":2395184,"user":{"displayName":"SHANEWAS AHMED NABIL","photoUrl":"","userId":"17864818584441031388"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import matplotlib.pyplot as plt\n","import PIL\n","import tensorflow as tf\n","import numpy as np\n","import os\n","\n","from tensorflow.python.keras.models import Model, Sequential\n","from tensorflow.python.keras.layers import Dense, Flatten, Dropout\n","from tensorflow.python.keras.applications.inception_v3 import InceptionV3\n","from tensorflow.python.keras.applications.inception_v3 import preprocess_input, decode_predictions\n","from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.python.keras.optimizers import Adam, RMSprop\n","\n","train_dir = \"/content/drive/My Drive/ADNI_C/test/des/ADtrain\"\n","test_dir = \"/content/drive/My Drive/ADNI_C/test/des/ADtest\"\n","\n","model = InceptionV3(include_top=False,input_shape=(224, 224, 3), weights='imagenet')\n","input_shape = (224,224)\n","\n","datagen_train = ImageDataGenerator(rescale=1./255)\n","datagen_test = ImageDataGenerator(rescale=1./255)\n","\n","\n","batch_size = 20\n","\n","generator_train = datagen_train.flow_from_directory(directory=train_dir,\n","                                                    target_size=input_shape,\n","                                                    batch_size=batch_size,\n","                                                    shuffle=True)\n","\n","generator_test = datagen_test.flow_from_directory(directory=test_dir,\n","                                                  target_size=input_shape,\n","                                                  batch_size=batch_size,\n","                                                  shuffle=False)\n","\n","steps_test = generator_test.n / batch_size\n","\n","\n","def path_join(dirname, filenames):\n","    return [os.path.join(dirname, filename) for filename in filenames]\n","\n","image_paths_train = path_join(train_dir, generator_train.filenames)\n","image_paths_test = path_join(test_dir, generator_test.filenames)\n","\n","cls_train = generator_train.classes\n","cls_test = generator_test.classes\n","\n","\n","class_names = list(generator_train.class_indices.keys())\n","num_classes = generator_train.num_classes\n","\n","transfer_layer = model.get_layer('mixed10')\n","conv_model = Model(inputs=model.input, outputs=transfer_layer.output)\n","\n","#for layer in conv_model.layers:\n","#    layer.trainable = False\n","    \n","# Start a new Keras Sequential model.\n","new_model = Sequential()\n","\n","# Add the convolutional part of the VGG16 model from above.\n","new_model.add(conv_model)\n","\n","# Flatten the output of the VGG16 model because it is from a\n","# convolutional layer.\n","new_model.add(Flatten())\n","\n","# Add a dense (aka. fully-connected) layer.\n","# This is for combining features that the VGG16 model has\n","# recognized in the image.\n","new_model.add(Dropout(0.5))\n","\n","new_model.add(Dense(1024, activation='relu'))\n","\n","# Add a dropout-layer which may prevent overfitting and\n","# improve generalization ability to unseen data e.g. the test-set.\n","#new_model.add(Dense(512, activation='relu'))\n","\n","\n","# Add the final layer for the actual classification.\n","new_model.add(Dense(num_classes, activation='softmax'))\n","\n","optimizer = Adam(lr=1e-5)\n","loss = 'categorical_crossentropy'\n","metrics = ['categorical_accuracy']\n","\n","\n","def print_layer_trainable():\n","    for layer in conv_model.layers:\n","        print(\"{0}:\\t{1}\".format(layer.trainable, layer.name))\n","\n","\n","print_layer_trainable()\n","\n","new_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n","\n","epochs = 20\n","steps_per_epoch = 100\n","\n","\n","history = new_model.fit_generator(generator=generator_train,\n","                                  epochs=epochs,\n","                                  steps_per_epoch=steps_per_epoch,\n","                                  validation_data=generator_test,\n","                                  validation_steps=steps_test)\n"," "],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0803 07:30:40.978742 140562427045760 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n"],"name":"stderr"},{"output_type":"stream","text":["Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n","87916544/87910968 [==============================] - 2s 0us/step\n","Found 6474 images belonging to 2 classes.\n","Found 1618 images belonging to 2 classes.\n","True:\tinput_1\n","True:\tconv2d\n","True:\tbatch_normalization\n","True:\tactivation\n","True:\tconv2d_1\n","True:\tbatch_normalization_1\n","True:\tactivation_1\n","True:\tconv2d_2\n","True:\tbatch_normalization_2\n","True:\tactivation_2\n","True:\tmax_pooling2d\n","True:\tconv2d_3\n","True:\tbatch_normalization_3\n","True:\tactivation_3\n","True:\tconv2d_4\n","True:\tbatch_normalization_4\n","True:\tactivation_4\n","True:\tmax_pooling2d_1\n","True:\tconv2d_8\n","True:\tbatch_normalization_8\n","True:\tactivation_8\n","True:\tconv2d_6\n","True:\tconv2d_9\n","True:\tbatch_normalization_6\n","True:\tbatch_normalization_9\n","True:\tactivation_6\n","True:\tactivation_9\n","True:\taverage_pooling2d\n","True:\tconv2d_5\n","True:\tconv2d_7\n","True:\tconv2d_10\n","True:\tconv2d_11\n","True:\tbatch_normalization_5\n","True:\tbatch_normalization_7\n","True:\tbatch_normalization_10\n","True:\tbatch_normalization_11\n","True:\tactivation_5\n","True:\tactivation_7\n","True:\tactivation_10\n","True:\tactivation_11\n","True:\tmixed0\n","True:\tconv2d_15\n","True:\tbatch_normalization_15\n","True:\tactivation_15\n","True:\tconv2d_13\n","True:\tconv2d_16\n","True:\tbatch_normalization_13\n","True:\tbatch_normalization_16\n","True:\tactivation_13\n","True:\tactivation_16\n","True:\taverage_pooling2d_1\n","True:\tconv2d_12\n","True:\tconv2d_14\n","True:\tconv2d_17\n","True:\tconv2d_18\n","True:\tbatch_normalization_12\n","True:\tbatch_normalization_14\n","True:\tbatch_normalization_17\n","True:\tbatch_normalization_18\n","True:\tactivation_12\n","True:\tactivation_14\n","True:\tactivation_17\n","True:\tactivation_18\n","True:\tmixed1\n","True:\tconv2d_22\n","True:\tbatch_normalization_22\n","True:\tactivation_22\n","True:\tconv2d_20\n","True:\tconv2d_23\n","True:\tbatch_normalization_20\n","True:\tbatch_normalization_23\n","True:\tactivation_20\n","True:\tactivation_23\n","True:\taverage_pooling2d_2\n","True:\tconv2d_19\n","True:\tconv2d_21\n","True:\tconv2d_24\n","True:\tconv2d_25\n","True:\tbatch_normalization_19\n","True:\tbatch_normalization_21\n","True:\tbatch_normalization_24\n","True:\tbatch_normalization_25\n","True:\tactivation_19\n","True:\tactivation_21\n","True:\tactivation_24\n","True:\tactivation_25\n","True:\tmixed2\n","True:\tconv2d_27\n","True:\tbatch_normalization_27\n","True:\tactivation_27\n","True:\tconv2d_28\n","True:\tbatch_normalization_28\n","True:\tactivation_28\n","True:\tconv2d_26\n","True:\tconv2d_29\n","True:\tbatch_normalization_26\n","True:\tbatch_normalization_29\n","True:\tactivation_26\n","True:\tactivation_29\n","True:\tmax_pooling2d_2\n","True:\tmixed3\n","True:\tconv2d_34\n","True:\tbatch_normalization_34\n","True:\tactivation_34\n","True:\tconv2d_35\n","True:\tbatch_normalization_35\n","True:\tactivation_35\n","True:\tconv2d_31\n","True:\tconv2d_36\n","True:\tbatch_normalization_31\n","True:\tbatch_normalization_36\n","True:\tactivation_31\n","True:\tactivation_36\n","True:\tconv2d_32\n","True:\tconv2d_37\n","True:\tbatch_normalization_32\n","True:\tbatch_normalization_37\n","True:\tactivation_32\n","True:\tactivation_37\n","True:\taverage_pooling2d_3\n","True:\tconv2d_30\n","True:\tconv2d_33\n","True:\tconv2d_38\n","True:\tconv2d_39\n","True:\tbatch_normalization_30\n","True:\tbatch_normalization_33\n","True:\tbatch_normalization_38\n","True:\tbatch_normalization_39\n","True:\tactivation_30\n","True:\tactivation_33\n","True:\tactivation_38\n","True:\tactivation_39\n","True:\tmixed4\n","True:\tconv2d_44\n","True:\tbatch_normalization_44\n","True:\tactivation_44\n","True:\tconv2d_45\n","True:\tbatch_normalization_45\n","True:\tactivation_45\n","True:\tconv2d_41\n","True:\tconv2d_46\n","True:\tbatch_normalization_41\n","True:\tbatch_normalization_46\n","True:\tactivation_41\n","True:\tactivation_46\n","True:\tconv2d_42\n","True:\tconv2d_47\n","True:\tbatch_normalization_42\n","True:\tbatch_normalization_47\n","True:\tactivation_42\n","True:\tactivation_47\n","True:\taverage_pooling2d_4\n","True:\tconv2d_40\n","True:\tconv2d_43\n","True:\tconv2d_48\n","True:\tconv2d_49\n","True:\tbatch_normalization_40\n","True:\tbatch_normalization_43\n","True:\tbatch_normalization_48\n","True:\tbatch_normalization_49\n","True:\tactivation_40\n","True:\tactivation_43\n","True:\tactivation_48\n","True:\tactivation_49\n","True:\tmixed5\n","True:\tconv2d_54\n","True:\tbatch_normalization_54\n","True:\tactivation_54\n","True:\tconv2d_55\n","True:\tbatch_normalization_55\n","True:\tactivation_55\n","True:\tconv2d_51\n","True:\tconv2d_56\n","True:\tbatch_normalization_51\n","True:\tbatch_normalization_56\n","True:\tactivation_51\n","True:\tactivation_56\n","True:\tconv2d_52\n","True:\tconv2d_57\n","True:\tbatch_normalization_52\n","True:\tbatch_normalization_57\n","True:\tactivation_52\n","True:\tactivation_57\n","True:\taverage_pooling2d_5\n","True:\tconv2d_50\n","True:\tconv2d_53\n","True:\tconv2d_58\n","True:\tconv2d_59\n","True:\tbatch_normalization_50\n","True:\tbatch_normalization_53\n","True:\tbatch_normalization_58\n","True:\tbatch_normalization_59\n","True:\tactivation_50\n","True:\tactivation_53\n","True:\tactivation_58\n","True:\tactivation_59\n","True:\tmixed6\n","True:\tconv2d_64\n","True:\tbatch_normalization_64\n","True:\tactivation_64\n","True:\tconv2d_65\n","True:\tbatch_normalization_65\n","True:\tactivation_65\n","True:\tconv2d_61\n","True:\tconv2d_66\n","True:\tbatch_normalization_61\n","True:\tbatch_normalization_66\n","True:\tactivation_61\n","True:\tactivation_66\n","True:\tconv2d_62\n","True:\tconv2d_67\n","True:\tbatch_normalization_62\n","True:\tbatch_normalization_67\n","True:\tactivation_62\n","True:\tactivation_67\n","True:\taverage_pooling2d_6\n","True:\tconv2d_60\n","True:\tconv2d_63\n","True:\tconv2d_68\n","True:\tconv2d_69\n","True:\tbatch_normalization_60\n","True:\tbatch_normalization_63\n","True:\tbatch_normalization_68\n","True:\tbatch_normalization_69\n","True:\tactivation_60\n","True:\tactivation_63\n","True:\tactivation_68\n","True:\tactivation_69\n","True:\tmixed7\n","True:\tconv2d_72\n","True:\tbatch_normalization_72\n","True:\tactivation_72\n","True:\tconv2d_73\n","True:\tbatch_normalization_73\n","True:\tactivation_73\n","True:\tconv2d_70\n","True:\tconv2d_74\n","True:\tbatch_normalization_70\n","True:\tbatch_normalization_74\n","True:\tactivation_70\n","True:\tactivation_74\n","True:\tconv2d_71\n","True:\tconv2d_75\n","True:\tbatch_normalization_71\n","True:\tbatch_normalization_75\n","True:\tactivation_71\n","True:\tactivation_75\n","True:\tmax_pooling2d_3\n","True:\tmixed8\n","True:\tconv2d_80\n","True:\tbatch_normalization_80\n","True:\tactivation_80\n","True:\tconv2d_77\n","True:\tconv2d_81\n","True:\tbatch_normalization_77\n","True:\tbatch_normalization_81\n","True:\tactivation_77\n","True:\tactivation_81\n","True:\tconv2d_78\n","True:\tconv2d_79\n","True:\tconv2d_82\n","True:\tconv2d_83\n","True:\taverage_pooling2d_7\n","True:\tconv2d_76\n","True:\tbatch_normalization_78\n","True:\tbatch_normalization_79\n","True:\tbatch_normalization_82\n","True:\tbatch_normalization_83\n","True:\tconv2d_84\n","True:\tbatch_normalization_76\n","True:\tactivation_78\n","True:\tactivation_79\n","True:\tactivation_82\n","True:\tactivation_83\n","True:\tbatch_normalization_84\n","True:\tactivation_76\n","True:\tmixed9_0\n","True:\tconcatenate\n","True:\tactivation_84\n","True:\tmixed9\n","True:\tconv2d_89\n","True:\tbatch_normalization_89\n","True:\tactivation_89\n","True:\tconv2d_86\n","True:\tconv2d_90\n","True:\tbatch_normalization_86\n","True:\tbatch_normalization_90\n","True:\tactivation_86\n","True:\tactivation_90\n","True:\tconv2d_87\n","True:\tconv2d_88\n","True:\tconv2d_91\n","True:\tconv2d_92\n","True:\taverage_pooling2d_8\n","True:\tconv2d_85\n","True:\tbatch_normalization_87\n","True:\tbatch_normalization_88\n","True:\tbatch_normalization_91\n","True:\tbatch_normalization_92\n","True:\tconv2d_93\n","True:\tbatch_normalization_85\n","True:\tactivation_87\n","True:\tactivation_88\n","True:\tactivation_91\n","True:\tactivation_92\n","True:\tbatch_normalization_93\n","True:\tactivation_85\n","True:\tmixed9_1\n","True:\tconcatenate_1\n","True:\tactivation_93\n","True:\tmixed10\n","Epoch 1/20\n","100/100 [==============================] - 873s 9s/step - loss: 0.8043 - categorical_accuracy: 0.6190 - val_loss: 0.6440 - val_categorical_accuracy: 0.6829\n","Epoch 2/20\n","100/100 [==============================] - 420s 4s/step - loss: 0.6087 - categorical_accuracy: 0.6975 - val_loss: 0.5648 - val_categorical_accuracy: 0.7064\n","Epoch 3/20\n","100/100 [==============================] - 458s 5s/step - loss: 0.5388 - categorical_accuracy: 0.7327 - val_loss: 0.5542 - val_categorical_accuracy: 0.7336\n","Epoch 4/20\n","100/100 [==============================] - 127s 1s/step - loss: 0.3625 - categorical_accuracy: 0.8345 - val_loss: 0.4638 - val_categorical_accuracy: 0.7769\n","Epoch 5/20\n","100/100 [==============================] - 31s 311ms/step - loss: 0.3105 - categorical_accuracy: 0.8601 - val_loss: 0.3316 - val_categorical_accuracy: 0.8449\n","Epoch 6/20\n","100/100 [==============================] - 31s 313ms/step - loss: 0.2653 - categorical_accuracy: 0.8825 - val_loss: 0.2403 - val_categorical_accuracy: 0.8993\n","Epoch 7/20\n","100/100 [==============================] - 31s 311ms/step - loss: 0.2155 - categorical_accuracy: 0.9100 - val_loss: 0.1831 - val_categorical_accuracy: 0.9203\n","Epoch 8/20\n","100/100 [==============================] - 31s 310ms/step - loss: 0.1531 - categorical_accuracy: 0.9433 - val_loss: 0.1720 - val_categorical_accuracy: 0.9302\n","Epoch 9/20\n","100/100 [==============================] - 31s 310ms/step - loss: 0.1559 - categorical_accuracy: 0.9360 - val_loss: 0.1629 - val_categorical_accuracy: 0.9339\n","Epoch 10/20\n","100/100 [==============================] - 31s 308ms/step - loss: 0.1505 - categorical_accuracy: 0.9385 - val_loss: 0.1703 - val_categorical_accuracy: 0.9314\n","Epoch 11/20\n","100/100 [==============================] - 31s 309ms/step - loss: 0.1003 - categorical_accuracy: 0.9585 - val_loss: 0.1164 - val_categorical_accuracy: 0.9549\n","Epoch 12/20\n","100/100 [==============================] - 31s 310ms/step - loss: 0.0952 - categorical_accuracy: 0.9630 - val_loss: 0.1446 - val_categorical_accuracy: 0.9456\n","Epoch 13/20\n","100/100 [==============================] - 31s 308ms/step - loss: 0.0873 - categorical_accuracy: 0.9644 - val_loss: 0.1080 - val_categorical_accuracy: 0.9586\n","Epoch 14/20\n","100/100 [==============================] - 31s 309ms/step - loss: 0.0741 - categorical_accuracy: 0.9720 - val_loss: 0.1098 - val_categorical_accuracy: 0.9549\n","Epoch 15/20\n","100/100 [==============================] - 31s 310ms/step - loss: 0.0578 - categorical_accuracy: 0.9775 - val_loss: 0.1097 - val_categorical_accuracy: 0.9524\n","Epoch 16/20\n","100/100 [==============================] - 31s 311ms/step - loss: 0.0522 - categorical_accuracy: 0.9814 - val_loss: 0.0981 - val_categorical_accuracy: 0.9604\n","Epoch 17/20\n","100/100 [==============================] - 31s 309ms/step - loss: 0.0388 - categorical_accuracy: 0.9865 - val_loss: 0.0741 - val_categorical_accuracy: 0.9697\n","Epoch 18/20\n","100/100 [==============================] - 31s 309ms/step - loss: 0.0577 - categorical_accuracy: 0.9780 - val_loss: 0.0731 - val_categorical_accuracy: 0.9740\n","Epoch 19/20\n","100/100 [==============================] - 31s 309ms/step - loss: 0.0379 - categorical_accuracy: 0.9880 - val_loss: 0.0654 - val_categorical_accuracy: 0.9759\n","Epoch 20/20\n","100/100 [==============================] - 31s 307ms/step - loss: 0.0418 - categorical_accuracy: 0.9840 - val_loss: 0.0862 - val_categorical_accuracy: 0.9710\n"],"name":"stdout"}]}]}